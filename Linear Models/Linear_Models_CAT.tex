% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Linear Models. Open Book CAT},
  pdfauthor={Getrude Gichuhi},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Linear Models. Open Book CAT}
\author{Getrude Gichuhi}
\date{08/04/2022}

\begin{document}
\maketitle

Loading the DataSet and viewing it

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'readxl' was built under R version 4.1.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"Cat1.xlsx"}\NormalTok{)}
\FunctionTok{View}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Split the data set into 75\% training set and 25\% test set.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 75\% of the sample size is the Training set }
\NormalTok{df\_t }\OtherTok{\textless{}{-}} \FunctionTok{floor}\NormalTok{(}\FloatTok{0.75} \SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(df))}

\CommentTok{\#setting the seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{training }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df)), }\AttributeTok{size =}\NormalTok{ df\_t)}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ df[training, ]}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ df[}\SpecialCharTok{{-}}\NormalTok{training, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{least-squares}{%
\subsection{Least squares}\label{least-squares}}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Fit a linear model using least squares on the training set, and report
  the test error obtained.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'caret' was built under R version 4.1.3
\end{verbatim}

\begin{verbatim}
## Loading required package: ggplot2
\end{verbatim}

\begin{verbatim}
## Loading required package: lattice
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'tidyverse' was built under R version 4.1.3
\end{verbatim}

\begin{verbatim}
## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --
\end{verbatim}

\begin{verbatim}
## v tibble  3.1.2     v dplyr   1.0.7
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   2.1.2     v forcats 0.5.1
## v purrr   0.3.4
\end{verbatim}

\begin{verbatim}
## Warning: package 'readr' was built under R version 4.1.3
\end{verbatim}

\begin{verbatim}
## Warning: package 'forcats' was built under R version 4.1.3
\end{verbatim}

\begin{verbatim}
## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
## x purrr::lift()   masks caret::lift()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'glmnet' was built under R version 4.1.3
\end{verbatim}

\begin{verbatim}
## Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'Matrix'
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:tidyr':
## 
##     expand, pack, unpack
\end{verbatim}

\begin{verbatim}
## Loaded glmnet 4.1-3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{=} \FunctionTok{lm}\NormalTok{(Response}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train)}

\FunctionTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = Response ~ ., data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.861  -7.204   0.419   7.927  24.937 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -413.2188    35.1070 -11.770  < 2e-16 ***
## Var1          26.6378     0.4928  54.049  < 2e-16 ***
## Var2          -6.1812     1.2884  -4.797 1.91e-06 ***
## Var3           4.7253     1.0729   4.404 1.20e-05 ***
## Var4           4.3928     1.1677   3.762 0.000181 ***
## Var5          -4.3347     0.6446  -6.724 3.34e-11 ***
## Var6           8.6981     1.2887   6.750 2.83e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 9.369 on 807 degrees of freedom
## Multiple R-squared:  0.9918, Adjusted R-squared:  0.9917 
## F-statistic: 1.626e+04 on 6 and 807 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Fitting training Model on the test set}
\NormalTok{lm\_pred}\OtherTok{=}\FunctionTok{predict}\NormalTok{(model,}\AttributeTok{new=}\NormalTok{test)}

\CommentTok{\#Calculating Accuracy }
\NormalTok{LSE}\OtherTok{=}\FunctionTok{mean}\NormalTok{((test}\SpecialCharTok{$}\NormalTok{Response}\SpecialCharTok{{-}}\NormalTok{lm\_pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\CommentTok{\#Print}
\FunctionTok{print}\NormalTok{(LSE)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 99.11668
\end{verbatim}

The Test error of the linear model fit is 99.116668

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Fit a ridge regression model on the training set, with λ chosen by
  cross-validation. Report the test error obtained.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{\#Matrices}
\NormalTok{train\_mat }\OtherTok{=} \FunctionTok{model.matrix}\NormalTok{(Response}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train)}
\NormalTok{test\_mat }\OtherTok{=} \FunctionTok{model.matrix}\NormalTok{(Response}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ test)}

\CommentTok{\#Choose the lambda using cross{-}validation}

\NormalTok{cv }\OtherTok{=} \FunctionTok{cv.glmnet}\NormalTok{(train\_mat, train}\SpecialCharTok{$}\NormalTok{Response, }\AttributeTok{alpha=}\DecValTok{0}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(cv)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Linear_Models_CAT_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lam }\OtherTok{=}\NormalTok{ cv}\SpecialCharTok{$}\NormalTok{lambda.min}

\NormalTok{lam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10.24674
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Fitting the ridge regression}

\NormalTok{ridge\_mod }\OtherTok{=} \FunctionTok{glmnet}\NormalTok{(train\_mat,train}\SpecialCharTok{$}\NormalTok{Response, }\AttributeTok{alpha =}\DecValTok{0}\NormalTok{)}

\CommentTok{\#Make Predictions }
\NormalTok{ridge\_pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(ridge\_mod, }\AttributeTok{s=}\NormalTok{lam,}\AttributeTok{newx =}\NormalTok{ test\_mat)}

\CommentTok{\#Calculating test error}
\FunctionTok{mean}\NormalTok{((ridge\_pred }\SpecialCharTok{{-}}\NormalTok{ test}\SpecialCharTok{$}\NormalTok{Response)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 224.7245
\end{verbatim}

The test error of the ridge regression fit with lambda chosen by
cross-validation is 224.7245, which is higher that the linear model
error.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Fit a lasso model on the training set, with λ chosen by cross
  validation. Report the test error obtained, along with the number of
  non-zero coefficient estimates.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Choosing the lambda to be used for the cross{-}validation }

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{cv2 }\OtherTok{=} \FunctionTok{cv.glmnet}\NormalTok{(train\_mat, train}\SpecialCharTok{$}\NormalTok{Response, }\AttributeTok{alpha=}\DecValTok{1}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(cv2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Linear_Models_CAT_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lam2 }\OtherTok{=}\NormalTok{ cv2}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{lam2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5597055
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Fitting the Lasso model}

\NormalTok{lasso }\OtherTok{=} \FunctionTok{glmnet}\NormalTok{(train\_mat, train}\SpecialCharTok{$}\NormalTok{Response, }\AttributeTok{alpha =}\DecValTok{1}\NormalTok{)}

\NormalTok{lasso\_1}\OtherTok{=} \FunctionTok{predict}\NormalTok{(lasso, }\AttributeTok{s=}\NormalTok{lam2, }\AttributeTok{newx=}\NormalTok{test\_mat)}

\FunctionTok{mean}\NormalTok{((lasso\_1 }\SpecialCharTok{{-}}\NormalTok{ test}\SpecialCharTok{$}\NormalTok{Response)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 106.7259
\end{verbatim}

The test error of the lasso model fit with a lambda chosen by
cross-validation is 106.7259. This error is between the least square
error slightly higher but lower than the ridge regression error.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Comment on the results obtained. How accurately can we predict the
  response variable? Is there much difference among the test errors
  resulting from these three approaches? Present and discuss results for
  the approaches
\end{enumerate}

The Model performance are as follows i) Linear Model using least square
error is 99.11668 ii) Ridge Regression with lambda chosen by
cross-validation is 224.7245 iii) Lasso model with lambda chosen by
cross validation is 106.7259

Therefore lasso model performs the best, while ridge regression model
performs the worst.

\end{document}
